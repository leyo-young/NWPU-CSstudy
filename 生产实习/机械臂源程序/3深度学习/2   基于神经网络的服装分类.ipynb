{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0358d263",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c1b9ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_workers = 4\n",
    "#下载训练集到指定目录\n",
    "mnist_train = torchvision.datasets.FashionMNIST(root='/home/retoo/Desktop/实验/数据集/3.深度学习/2FashionMNIST', train=True, download=True, transform=transforms.ToTensor())\n",
    "# 下载测试集到指定目录\n",
    "mnist_test = torchvision.datasets.FashionMNIST(root='/home/retoo/Desktop/实验/数据集/3.深度学习/2FashionMNIST', train=False, download=True,transform=transforms.ToTensor())\n",
    "# 将训练集组成一个批次\n",
    "train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "# 将测试集组成一个批次\n",
    "test_iter=torch.utils.data.DataLoader(mnist_test,batch_size=batch_size,shuffle=True,num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6284a857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 9\n"
     ]
    }
   ],
   "source": [
    "feature, label = mnist_train[0]#读取第一个获得的训练数据，每一个训练数据均为data与label\n",
    "print(feature.shape, label)  #每一个data为三通道数据,label为单通道标签数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cea70c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):#定义了激活函数\n",
    "    return torch.max(input=X, other=torch.tensor(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efe3b8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):#通过softmax函数，计算概率\n",
    "    X_exp = X.exp()\n",
    "    partition = X_exp.sum(dim=1, keepdim=True)\n",
    "    return X_exp / partition  # 这里应用了广播机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9864cd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
    "\n",
    "W1 = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_hiddens)), dtype=torch.float)#采用初始化参数\n",
    "b1 = torch.zeros(num_hiddens, dtype=torch.float)#采用零值初始化偏差\n",
    "W2 = torch.tensor(np.random.normal(0, 0.01, (num_hiddens, num_outputs)), dtype=torch.float)\n",
    "b2 = torch.zeros(num_outputs, dtype=torch.float)#采用零值初始化偏差\n",
    "\n",
    "params = [W1, b1, W2, b2]#参数所构成的列表\n",
    "for param in params:\n",
    "    param.requires_grad_(requires_grad=True)#设置所有的参数列表中的参数均需要进行反向求导\n",
    "\n",
    "\n",
    "def net(X):#定义网络结构\n",
    "    X = X.view((-1, num_inputs))#将输入的图像数据展平为单列的784的数据，有利于全连接层的使用\n",
    "    H = relu(torch.matmul(X, W1) + b1)#将Ｈ＝X*W1+b1，再求激活函数\n",
    "    O = softmax(torch.matmul(H, W2) + b2)#将Ｏ＝Ｈ*Ｗ２＋b2，再用softmax进行分类\n",
    "    return O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c41429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y):#定义交叉熵损失函数\n",
    "    return - torch.log(y_hat.gather(1, y.view(-1, 1)))\n",
    "    #tensor.gather与torch.gather用法一样，\n",
    "    # tensor.gather(dim,indexs)，在dim维度上，安装indexs所给的坐标选择元素，返回一个和index维度相同的tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd65f093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):#参数优化方法\n",
    "    for param in params:#所有需要优化的参数\n",
    "        param.data -= lr * param.grad / batch_size # 由于param是tensor格式，因此更改param参数数据时用的param.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "219be140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net):#精度评价\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for X, y in data_iter:\n",
    "        acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()#dim=1表示按行，取概率最大的，由于一次X包括了batch个数据，\n",
    "        # 因此，把数据求和．.item()取出单元素张量的元素值并返回该值，保持原元素类型不变\n",
    "        n += y.shape[0]\n",
    "    return acc_sum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2004af95",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, lr = 5, 0.1#定义训练的次数与学习率\n",
    "\n",
    "def train_ch2(net, train_iter, test_iter, loss, num_epochs, batch_size,\n",
    "              params=None, lr=None, optimizer=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0#每次训练对上次训练的结果进行归零\n",
    "        for X, y in train_iter:\n",
    "            y_hat = net(X)#\n",
    "            l = loss(y_hat, y).sum()#l要求偏导，是tensor格式\n",
    "\n",
    "            # 梯度清零\n",
    "            if optimizer is not None:#如果定义了特定的参数优化器，则将参数优化器中的参数设置为０．本部分在使用的时候由于没有定义，所以不会执行下面的清零\n",
    "                optimizer.zero_grad()\n",
    "            elif params is not None and params[0].grad is not None:#如果参数有定义并且参数第一个值的梯度也有定义，则执行下面的清零\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "\n",
    "            l.backward()#梯度反向求导\n",
    "            if optimizer is None:\n",
    "                sgd(params, lr, batch_size)#没有定义特殊的优化器，则使用默认的SGD,本实验采用的是sgd\n",
    "            else:\n",
    "                optimizer.step()#定义了特殊的优化器，则直接用优化器step进行参数的学习\n",
    "\n",
    "\n",
    "            train_l_sum += l.item()#把所有损失函数的值相加，即batch=256个值相加起来\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()#精度是由预测出的概率最大项与标签之间对比相同，\n",
    "            n += y.shape[0]#即为batch值\n",
    "        test_acc = evaluate_accuracy(test_iter, net)#验证数据集的精度，求解的方法与训练的数据集方法一样\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))#输出训练的次数，平均损失函数的大小，训练精度，测试精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79e76f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.0443, train acc 0.645, test acc 0.737\n",
      "epoch 2, loss 0.5993, train acc 0.789, test acc 0.799\n",
      "epoch 3, loss 0.5196, train acc 0.817, test acc 0.822\n"
     ]
    }
   ],
   "source": [
    "train_ch2(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size, params, lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
