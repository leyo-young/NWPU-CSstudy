{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "526d5303-0c8c-40fa-920d-df7876cc1d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a39809d-12b7-4f0f-8e87-307d5e32a942",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_workers = 4\n",
    "#下载训练集到指定目录\n",
    "mnist_train = torchvision.datasets.FashionMNIST(root='/home/retoo/Desktop/实验/数据集/3 深度学习/3FashionMNIST', train=True, download=True, transform=transforms.ToTensor())\n",
    "# 下载测试集到指定目录\n",
    "mnist_test = torchvision.datasets.FashionMNIST(root='/home/retoo/Desktop/实验/数据集/3 深度学习/3FashionMNIST', train=False, download=True, transform=transforms.ToTensor())\n",
    "# 将训练集组成一个批次\n",
    "train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "# 将测试集组成一个批次\n",
    "test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "243ec876-7374-4761-9434-c337dc4998db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 9\n"
     ]
    }
   ],
   "source": [
    "feature, label = mnist_train[0]#读取第一个获得的训练数据，每一个训练数据均为data与label\n",
    "print(feature.shape, label)  # Channel x Height x Width　＃每一个data为三通道数据,label为单通道标签数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cff8851-a4d1-40e8-ba4c-88c3b04b7d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(X, drop_prob):#定义droppout模块\n",
    "    X = X.float()\n",
    "    assert 0 <= drop_prob <= 1#设置丢失的概率，如果不是０~1,则系统设置错误\n",
    "    keep_prob = 1 - drop_prob\n",
    "    # 这种情况下把全部元素都丢弃\n",
    "    if keep_prob == 0:\n",
    "        return torch.zeros_like(X)#如果丢失率为１００％,则返回的值全部为零\n",
    "    # print(X.shape)\n",
    "    # print(X.sum().item())\n",
    "    mask = (torch.rand(X.shape) < keep_prob).float()  # 丢失概率不为全部，X.shape([2,8]),随机生产一个０－1的形状与Ｘ相同的，小于０．５的为１，否则为０\n",
    "    # 通过ｍａｓｋ＊Ｘ，保留１位置的值/home/retoo/Desktop/实验/数据集/3.深度学习/3FashionMNIST', train=False, download=True, transform=transforms.ToTensor())\n",
    "    \n",
    "    # 为了维持训练和测试的时候输出的期望保持一致，在代码上有两种做法，一种是在训练的时候对输出除以1-p（保留的概率），另外一种则是在预测的时候对输出乘以p\n",
    "\n",
    "    return mask * X / keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d9f8921-4be6-463c-b6ee-b82d941b0cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.,  6.,  8.,  0., 12., 14.],\n",
       "        [16., 18.,  0., 22., 24.,  0.,  0., 30.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(16).view(2, 8)#假设输入的图像为１６个，将其resize到(2,8)\n",
    "dropout(X, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "099c812c-79ea-4685-8458-605c2689ce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256#设置各层的网络节点数\n",
    "W1 = torch.tensor(np.random.normal(0, 0.01, size=(num_inputs, num_hiddens1)), dtype=torch.float, requires_grad=True)\n",
    "#W1为第一层的参数，输入节点与第一层隐含层之间的参数\n",
    "b1 = torch.zeros(num_hiddens1, requires_grad=True)#第一层的偏置量\n",
    "W2 = torch.tensor(np.random.normal(0, 0.01, size=(num_hiddens1, num_hiddens2)), dtype=torch.float, requires_grad=True)\n",
    "#W２为第二层的参数，第一层隐含层与第二层隐含层之间的参数\n",
    "b2 = torch.zeros(num_hiddens2, requires_grad=True)#第二层的偏置量\n",
    "W3 = torch.tensor(np.random.normal(0, 0.01, size=(num_hiddens2, num_outputs)), dtype=torch.float, requires_grad=True)\n",
    "#W３为第三层的参数，第二层隐含层与输出层之间的参数\n",
    "b3 = torch.zeros(num_outputs, requires_grad=True)#第三层的偏置量\n",
    "params = [W1, b1, W2, b2, W3, b3]#参数列表，但是每个参数均为tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01260c2c-2764-46a0-9039-131dbf7a03b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_prob1, drop_prob2 = 0.2, 0.5#不同的droppout比例\n",
    "def net(X, is_training=True):\n",
    "    X = X.view(-1, num_inputs)#将输入展平为一维的\n",
    "    H1 = (torch.matmul(X, W1) + b1).relu()\n",
    "    if is_training:  # 只在训练模型时使用丢弃法\n",
    "        H1 = dropout(H1, drop_prob1)  # 在第一层全连接后添加丢弃层\n",
    "    H2 = (torch.matmul(H1, W2) + b2).relu()\n",
    "    if is_training:\n",
    "        H2 = dropout(H2, drop_prob2)  # 在第二层全连接后添加丢弃层\n",
    "    return torch.matmul(H2, W3) + b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aae7504d-d40a-4972-90af-3be84f1fcd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()#损失函数直接选择torch.nn中的交叉熵损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "756231bd-dc30-4406-b1cf-c885f66e43bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):\n",
    "    for param in params:\n",
    "        param.data -= lr * param.grad / batch_size # 注意这里更改param时用的param.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d892d61e-1ee9-4863-bdd7-8ad054e5aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for X, y in data_iter:\n",
    "        if isinstance(net, torch.nn.Module):#如果net模型为基于torch.nn.Module搭建的，深度学习的常用模型\n",
    "            net.eval() # 评估模式, 这会关闭dropout\n",
    "            acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\n",
    "            net.train() # 改回训练模式\n",
    "        else: # 自定义的模型\n",
    "            if('is_training' in net.__code__.co_varnames): # 如果有is_training这个参数\n",
    "                #net.__code__.co_varnames 将函数的局部变量以元组的形式返回，除此以外，还有fun.__code__.co_argcount返回函数中参数的个数等（*args以前的）\n",
    "                # 将is_training设置成False\n",
    "                #?为什么在精度计算的过程中需要将网络修改为验证模式\n",
    "                acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item()\n",
    "            else:\n",
    "                acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52a63db7-0443-4f21-98b0-b4885ccac31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, lr = 5, 0.1\n",
    "\n",
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,\n",
    "              params=None, lr=None, optimizer=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0#训练的时候，每个epoch都需要对其进行清零\n",
    "        for X, y in train_iter:\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y).sum()#l还是为tensor，值是一个．\n",
    "\n",
    "            # 梯度清零\n",
    "            if optimizer is not None:#如果定义了优化器\n",
    "                optimizer.zero_grad()\n",
    "            elif params is not None and params[0].grad is not None:#如果定义了参数\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "            l.backward()#梯度反向求导\n",
    "            if optimizer is None:\n",
    "                sgd(params, lr, batch_size)#没有定义优化器，用自己定义的sgd求解参数\n",
    "            else:\n",
    "                optimizer.step()  #优化器为torch中提供的，则用.step()\n",
    "            train_l_sum += l.item()#每次生成的batch个损失函数，在一次epoch中集中计算一次\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()#在每次的预测输出中，通过概率最大值与标签值之间的比较\n",
    "            n += y.shape[0]#batch值\n",
    "        test_acc = evaluate_accuracy(test_iter, net)#验证集数据的精度计算\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17dc177b-cad0-4b93-bc74-96e02d6c7d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.0046, train acc 0.554, test acc 0.739\n",
      "epoch 2, loss 0.0023, train acc 0.785, test acc 0.739\n",
      "epoch 3, loss 0.0019, train acc 0.823, test acc 0.811\n",
      "epoch 4, loss 0.0017, train acc 0.839, test acc 0.825\n",
      "epoch 5, loss 0.0016, train acc 0.847, test acc 0.792\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 5, 100.0\n",
    "train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)\n",
    "#对比实验２，本实验中损失函数更小，训练的精度也要更高些．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c13c933-3b82-455c-b87c-211a924c833b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
