{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cca325",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyTorch用的包\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "#from torch.autograd import Variable\n",
    "# 自然语言处理相关的包\n",
    "import re #正则表达式的包\n",
    "import jieba #结巴分词包\n",
    "from collections import Counter #搜集器，可以让统计词频更简单\n",
    "#绘图、计算用的程序包\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ced950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据来源文件\n",
    "good_file = '/home/retoo/Desktop/实验/数据集/3.深度学习/8evaluation_data/good.txt'\n",
    "bad_file  = '/home/retoo/Desktop/实验/数据集/3.深度学习/8evaluation_data/bad.txt'\n",
    "\n",
    "# 将文本中的标点符号过滤掉\n",
    "def filter_punc(sentence):\n",
    "    sentence = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\'“”《》?“]+|[+——！，。？、~@#￥%……&*（）：]+\", \"\", sentence)  \n",
    "    return(sentence)\n",
    "#扫描所有的文本，分词、建立词典，分出正向还是负向的评论，is_filter可以过滤是否筛选掉标点符号\n",
    "def Prepare_data(good_file, bad_file, is_filter = True):\n",
    "    all_words = [] #存储所有的单词\n",
    "    pos_sentences = [] #存储正向的评论\n",
    "    neg_sentences = [] #存储负向的评论\n",
    "    with open(good_file,'r') as fr:\n",
    "        # with open(good_file，'r') as fr:\n",
    "        for idx, line in enumerate(fr):\n",
    "            if is_filter:\n",
    "                #过滤标点符号\n",
    "                line = filter_punc(line)\n",
    "            #分词\n",
    "            words = jieba.lcut(line)\n",
    "            if len(words) > 0:\n",
    "                all_words += words\n",
    "                pos_sentences.append(words)\n",
    "    print('{0} 包含 {1} 行, {2} 个词.'.format(good_file, idx+1, len(all_words)))\n",
    "\n",
    "    count = len(all_words)\n",
    "    with open(bad_file,'r') as fr:\n",
    "        for idx, line in enumerate(fr):\n",
    "            if is_filter:\n",
    "                line = filter_punc(line)\n",
    "            words = jieba.lcut(line)\n",
    "            if len(words) > 0:\n",
    "                all_words += words\n",
    "                neg_sentences.append(words)\n",
    "    print('{0} 包含 {1} 行, {2} 个词.'.format(bad_file, idx+1, len(all_words)-count))\n",
    "\n",
    "    #建立词典，diction的每一项为{w:[id, 单词出现次数]}\n",
    "    diction = {}\n",
    "    cnt = Counter(all_words)\n",
    "    for word, freq in cnt.items():\n",
    "        diction[word] = [len(diction), freq]\n",
    "    print('字典大小：{}'.format(len(diction)))\n",
    "    return(pos_sentences, neg_sentences, diction)\n",
    "\n",
    "#根据单词返还单词的编码\n",
    "def word2index(word, diction):\n",
    "    if word in diction:\n",
    "        value = diction[word][0]\n",
    "    else:\n",
    "        value = -1\n",
    "    return(value)\n",
    "#根据编码获得单词\n",
    "def index2word(index, diction):\n",
    "    for w,v in diction.items():\n",
    "        if v[0] == index:\n",
    "            return(w)\n",
    "    return(None)\n",
    "\n",
    "pos_sentences, neg_sentences, diction = Prepare_data(good_file, bad_file, True)\n",
    "st = sorted([(v[1], w) for w, v in diction.items()])\n",
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582ed5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入一个句子和相应的词典，得到这个句子的向量化表示\n",
    "# 向量的尺寸为词典中词汇的个数，i位置上面的数值为第i个单词在sentence中出现的频率\n",
    "def sentence2vec(sentence, dictionary):\n",
    "    vector = np.zeros(len(dictionary))\n",
    "    for l in sentence:\n",
    "        vector[l] += 1\n",
    "    return(1.0 * vector / len(sentence))\n",
    "\n",
    "# 遍历所有句子，将每一个词映射成编码\n",
    "dataset = [] #数据集\n",
    "labels = [] #标签\n",
    "sentences = [] #原始句子，调试用\n",
    "# 处理正向评论\n",
    "for sentence in pos_sentences:\n",
    "    new_sentence = []\n",
    "    for l in sentence:\n",
    "        if l in diction:\n",
    "            new_sentence.append(word2index(l, diction))\n",
    "    dataset.append(sentence2vec(new_sentence, diction))\n",
    "    labels.append(0) #正标签为0\n",
    "    sentences.append(sentence)\n",
    "\n",
    "# 处理负向评论\n",
    "for sentence in neg_sentences:\n",
    "    new_sentence = []\n",
    "    for l in sentence:\n",
    "        if l in diction:\n",
    "            new_sentence.append(word2index(l, diction))\n",
    "    dataset.append(sentence2vec(new_sentence, diction))\n",
    "    labels.append(1) #负标签为1\n",
    "    sentences.append(sentence)\n",
    "#打乱所有的数据顺序，形成数据集\n",
    "# indices为所有数据下标的一个全排列\n",
    "indices = np.random.permutation(len(dataset))\n",
    "#重新根据打乱的下标生成数据集dataset，标签集labels，以及对应的原始句子sentences\n",
    "dataset = [dataset[i] for i in indices]\n",
    "labels = [labels[i] for i in indices]\n",
    "sentences = [sentences[i] for i in indices]\n",
    "#对整个数据集进行划分，分为：训练集、校准集和测试集，其中校准和测试集合的长度都是整个数据集的10分之一\n",
    "test_size = len(dataset) // 10\n",
    "train_data = dataset[2 * test_size :]\n",
    "train_label = labels[2 * test_size :]\n",
    "valid_data = dataset[: test_size]\n",
    "valid_label = labels[: test_size]\n",
    "test_data = dataset[test_size : 2 * test_size]\n",
    "test_label = labels[test_size : 2 * test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ba028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个简单的前馈神经网络，三层，第一层线性层，加一个非线性ReLU，第二层线性层，中间有10个隐含层神经元\n",
    "# 输入维度为词典的大小：每一段评论的词袋模型\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(len(diction), 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 2),\n",
    "    nn.LogSoftmax(dim=1),\n",
    ")\n",
    "def rightness(predictions, labels):\n",
    "    \"\"\"计算预测错误率的函数，其中predictions是模型给出的一组预测结果，batch_size行num_classes列的矩阵，labels是数据之中的正确答案\"\"\"\n",
    "    pred = torch.max(predictions.data, 1)[1] # 对于任意一行（一个样本）的输出值的第1个维度，求最大，得到每一行的最大元素的下标\n",
    "    rights = pred.eq(labels.data.view_as(pred)).sum() #将下标与labels中包含的类别进行比较，并累计得到比较正确的数量\n",
    "    return rights, len(labels) #返回正确的数量和这一次一共比较了多少元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fc8d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数为交叉熵\n",
    "cost = torch.nn.NLLLoss()\n",
    "# 优化算法为Adam，可以自动调节学习率\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "records = []\n",
    "#循环10个Epoch\n",
    "losses = []\n",
    "for epoch in range(10):\n",
    "    for i, data in enumerate(zip(train_data, train_label)):\n",
    "        x, y = data        \n",
    "        # 需要将输入的数据进行适当的变形，主要是要多出一个batch_size的维度，也即第一个为1的维度\n",
    "        x = torch.tensor(x, requires_grad = True, dtype = torch.float).view(1,-1)\n",
    "        # x的尺寸：batch_size=1, len_dictionary\n",
    "        # 标签也要加一层外衣以变成1*1的张量\n",
    "        y = torch.tensor(np.array([y]), dtype = torch.long)\n",
    "        # y的尺寸：batch_size=1, 1\n",
    "        # 清空梯度\n",
    "        optimizer.zero_grad()\n",
    "        # 模型预测\n",
    "        predict = model(x)\n",
    "        # 计算损失函数\n",
    "        loss = cost(predict, y)\n",
    "        # 将损失函数数值加入到列表中\n",
    "        losses.append(loss.data.numpy())\n",
    "        # 开始进行梯度反传\n",
    "        loss.backward()\n",
    "        # 开始对参数进行一步优化\n",
    "        optimizer.step()\n",
    "        # 每隔3000步，跑一下校验数据集的数据，输出临时结果\n",
    "        if i % 3000 == 0:\n",
    "            val_losses = []\n",
    "            rights = []\n",
    "            # 在所有校验数据集上实验\n",
    "            for j, val in enumerate(zip(valid_data, valid_label)):\n",
    "                x, y = val\n",
    "                x = torch.tensor(x, requires_grad = True, dtype = torch.float).view(1,-1)\n",
    "                y = torch.tensor(np.array([y]), dtype = torch.long)\n",
    "                predict = model(x)\n",
    "                # 调用rightness函数计算准确度\n",
    "                right = rightness(predict, y)\n",
    "                rights.append(right)\n",
    "                loss = cost(predict, y)\n",
    "                val_losses.append(loss.data.numpy())\n",
    "            # 将校验集合上面的平均准确度计算出来\n",
    "            right_ratio = 1.0 * np.sum([i[0] for i in rights]) / np.sum([i[1] for i in rights])\n",
    "            print('第{}轮, 训练损失: {:.2f}, 校验损失: {:.2f}, 校验准确率: {:.2f}'.format(epoch, np.mean(losses), np.mean(val_losses), right_ratio))\n",
    "            records.append([np.mean(losses), np.mean(val_losses), right_ratio])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4062811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制误差曲线\n",
    "a = [i[0] for i in records]\n",
    "b = [i[1] for i in records]\n",
    "c = [i[2] for i in records]\n",
    "plt.plot(a, label = 'Train Loss')\n",
    "plt.plot(b, label = 'Valid Loss')\n",
    "plt.plot(c, label = 'Valid Accuracy')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss & Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5e87c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = [] #记录准确率所用列表\n",
    "#对测试数据集进行循环\n",
    "for data, target in zip(test_data, test_label):\n",
    "    data, target = torch.tensor(data, dtype = torch.float).view(1,-1), torch.tensor(np.array([target]), dtype = torch.long)\n",
    "    output = model(data) #将特征数据喂入网络，得到分类的输出\n",
    "    val = rightness(output, target) #获得正确样本数以及总样本数\n",
    "    vals.append(val) #记录结果\n",
    "#计算准确率\n",
    "rights = (sum([tup[0] for tup in vals]), sum([tup[1] for tup in vals]))\n",
    "right_rate = 1.0 * rights[0].data.numpy() / rights[1]\n",
    "right_rate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
